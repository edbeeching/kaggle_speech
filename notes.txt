#######################
## FIRST SUBMISSIONS ## 

sample submission achieves 0.09 accuracy, indicating that approximately 9% of the dataset is silence ~ 1/12

first real submission with ~ 1/3 unknown yields 81% accuracy, indicating strong prior of unknown


modified weighting of prior on weighting vector [-0.4,-0.4,-0.4,-0.4,-0.4,-0.4,-0.4,-0.4, 0.4,-0.4,-0.4,-0.4] 
achieves accuracy of 0.72
probably need to slightly weight the prior of silence as it is known that 9% of the data is silence

subtrcting weighting of prior on weighting vector [-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2, 0.2,-0.2,-0.2,-0.2]
achieves accuracy of 0.8


sumbitting all unknowns achieves accuracy of 0.09 this is strange as many examples are unknown



########################
## Batch of test 04j01
64 neurons a reg of 0.0001 performs best
128 neurons a reg of 0.0001 performs best
256 neurons a reg of 0.0001 performs best
512 neurons a reg of 0.0001 performs best

comparing 64, 128, 256 and 512:
    256 and 512 achieve similar performance
    
make submission with 256, accuracy is lower: 80%

follow up with tests of reg = 0.00001 for the tests listed above

Ideas:
    Recurrent regularization
    bi directionality
    Preprocessing, make small adjustments
    
#####
## Submission with recurrent dropout of 0.3
with        0.81
without     0.80

########
## Next test to try

1. Preproc in classifer
2. Fully conv classifier

#######
## Fully conv classifer:
    best result on validation is with reg=0.0001, drop = 0.4
    should try training for longer, 20 epochs?
    will try sumbitting for reg = 0.0 and reg=0.0001 both with drop 0.4
    accuracy for 0.0 = 0.77
    accuracy for 0.0001 = 0.80
    
    clearly a fully convolutional net has promise
    
######
## Next to try:
    1. train for longer - No real improvement, training with higher dropout and more regularisation
    2. deeper?
    3. Conv + LSTM?
    4. preproc, frame size too small?
    5. More simple model
    6. data augmentation
    
######
## Training for longer for with more regularization:
    0.0001 with dropout of 0.8 appears to provide the best result, after submission with class weights, score = 0.77

######
## Try the smallest network possible, as there is not much data

######
## Smallest network with 

    model_input= Input(shape=input_shape)
    x = Conv2D(16, kernel_size=(3,3), activation='relu')(model_input)
    x = MaxPool2D(pool_size=(2,2))(x)
    x = Conv2D(32, kernel_size=(3,3), activation='relu')(x)
    x = MaxPool2D(pool_size=(2,2))(x)
    x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)
    x = MaxPool2D(pool_size=(2,2))(x)
    x = Conv2D(32, kernel_size=(1,6),activation='relu')(x)
    x = Flatten()(x)
    x = Dropout(drop)(x)
    model_output = Dense(12, activation='softmax', kernel_regularizer=l2(reg))(x)
    
10 randomly selected tests:
best on valid is :     reg=0.0001 drop=0.4 weights=False    with 0.924 accuracy on validation set

#####
## slightly less small network have best parameters:
    reg 0.001, drop 0.8 wieghts = True  with 0.9059 accuracy on valdation set

#####
## next is small conv net + LSTM
best result is with reg = 0.0 drop = 0.8








