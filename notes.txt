#######################
## FIRST SUBMISSIONS ## 

sample submission achieves 0.09 accuracy, indicating that approximately 9% of the dataset is silence ~ 1/12

first real submission with ~ 1/3 unknown yields 81% accuracy, indicating strong prior of unknown


modified weighting of prior on weighting vector [-0.4,-0.4,-0.4,-0.4,-0.4,-0.4,-0.4,-0.4, 0.4,-0.4,-0.4,-0.4] 
achieves accuracy of 0.72
probably need to slightly weight the prior of silence as it is known that 9% of the data is silence

subtrcting weighting of prior on weighting vector [-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2,-0.2, 0.2,-0.2,-0.2,-0.2]
achieves accuracy of 0.8


sumbitting all unknowns achieves accuracy of 0.09 this is strange as many examples are unknown



########################
## Batch of test 04j01
64 neurons a reg of 0.0001 performs best
128 neurons a reg of 0.0001 performs best
256 neurons a reg of 0.0001 performs best
512 neurons a reg of 0.0001 performs best

comparing 64, 128, 256 and 512:
    256 and 512 achieve similar performance
    
make submission with 256, accuracy is lower: 80%

follow up with tests of reg = 0.00001 for the tests listed above

Ideas:
    Recurrent regularization
    bi directionality
    Preprocessing, make small adjustments
    
#####
## Submission with recurrent dropout of 0.3
with        0.81
without     0.80

########
## Next test to try

1. Preproc in classifer
2. Fully conv classifier

#######
## Fully conv classifer:
    best result on validation is with reg=0.0001, drop = 0.4
    should try training for longer, 20 epochs?
    will try sumbitting for reg = 0.0 and reg=0.0001 both with drop 0.4
    accuracy for 0.0 = 0.77
    accuracy for 0.0001 = 0.80
    
    clearly a fully convolutional net has promise
    
######
## Next to try:
    1. train for longer - No real improvement, training with higher dropout and more regularisation
    2. deeper?
    3. Conv + LSTM?
    4. preproc, frame size too small?
    5. More simple model
    
######
## Training for longer for with more regularization:
    0.0001 with dropout of 0.8 appears to provide the best result






